{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there will be more packages added as needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#note this package is NOT native to Anaconda, follow tabula\n",
    "#documentation for steps on installation\n",
    "import tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv files (all links were busted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF WEBSCRAPING\n",
    "\n",
    "link to example pdf:\n",
    "\n",
    "http://cdfdata.fire.ca.gov/pub/cdf/images/incidentstatsevents_272.pdf\n",
    "\n",
    "\n",
    "another link to explore later:\n",
    "\n",
    "https://www.usfa.fema.gov/data/statistics/ \n",
    "\n",
    "\n",
    "another link with firestation locations:\n",
    "\n",
    "https://hifld-geoplatform.opendata.arcgis.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will scrape some data directly from pdf using tabula open source pdf reader.\n",
    "\n",
    "#variable format largest_[year]_[page]\n",
    "#2012 link is broken\n",
    "# (do this later) TODO: drop anything that is missing a date\n",
    "raw_fire_array = []\n",
    "#2000 2 pages\n",
    "largest_2000_1 = tabula.read_pdf('largest_2000.pdf', pages=1, area=[60, 105, 1004, 800])\n",
    "raw_fire_array.append(largest_2000_1)\n",
    "largest_2000_2 = tabula.read_pdf('largest_2000.pdf', pages=2, area=[60, 51, 1004, 650])\n",
    "raw_fire_array.append(largest_2000_2)\n",
    "#2001 2 pages\n",
    "largest_2001_1 = tabula.read_pdf('largest_2001.pdf', pages=1, area=[100, 88, 952, 792])\n",
    "raw_fire_array.append(largest_2001_1)\n",
    "largest_2001_2 = tabula.read_pdf('largest_2001.pdf', pages=2, area=[100, 46, 952, 623])\n",
    "raw_fire_array.append(largest_2001_2)\n",
    "#2002 3 pages\n",
    "largest_2002_1 = tabula.read_pdf('largest_2002.pdf', pages=1, area=[76, 104, 1010, 797])\n",
    "raw_fire_array.append(largest_2002_1)\n",
    "largest_2002_2 = tabula.read_pdf('largest_2002.pdf', pages=2, area=[76, 50, 1010, 786])\n",
    "raw_fire_array.append(largest_2002_2)\n",
    "largest_2002_3 = tabula.read_pdf('largest_2002.pdf', pages=3, area=[76, 50, 1010, 681])\n",
    "raw_fire_array.append(largest_2002_3)\n",
    "#2003 3 pages\n",
    "largest_2003_1 = tabula.read_pdf('largest_2003.pdf', pages=1, area=[58, 81, 1015, 787])\n",
    "raw_fire_array.append(largest_2003_1)\n",
    "largest_2003_2 = tabula.read_pdf('largest_2003.pdf', pages=2, area=[58, 33, 1015, 784])\n",
    "raw_fire_array.append(largest_2003_2)\n",
    "largest_2003_3 = tabula.read_pdf('largest_2003.pdf', pages=3, area=[58, 33, 1015, 169])\n",
    "raw_fire_array.append(largest_2003_3)\n",
    "#2004 2 pages\n",
    "largest_2004_1 = tabula.read_pdf('largest_2004.pdf', pages=1, area=[81, 125, 1017, 797])\n",
    "raw_fire_array.append(largest_2004_1)\n",
    "largest_2004_2 = tabula.read_pdf('largest_2004.pdf', pages=2, area=[81, 76, 1017, 646])\n",
    "raw_fire_array.append(largest_2004_2)\n",
    "#2005 3 pages\n",
    "largest_2005_1 = tabula.read_pdf('largest_2005.pdf', pages=1, area=[110, 86, 958, 793])\n",
    "raw_fire_array.append(largest_2005_1)\n",
    "largest_2005_2 = tabula.read_pdf('largest_2005.pdf', pages=2, area=[110, 86, 958, 793])\n",
    "raw_fire_array.append(largest_2005_2)\n",
    "largest_2005_3 = tabula.read_pdf('largest_2005.pdf', pages=3, area=[110, 86, 958, 385])\n",
    "raw_fire_array.append(largest_2005_3)\n",
    "#2006 4 pages\n",
    "largest_2006_1 = tabula.read_pdf('largest_2006.pdf', pages=1, area=[64, 90, 1005, 806])\n",
    "raw_fire_array.append(largest_2006_1)\n",
    "largest_2006_2 = tabula.read_pdf('largest_2006.pdf', pages=2, area=[64, 90, 1005, 806])\n",
    "raw_fire_array.append(largest_2006_2)\n",
    "largest_2006_3 = tabula.read_pdf('largest_2006.pdf', pages=3, area=[64, 90, 1005, 806])\n",
    "raw_fire_array.append(largest_2006_3)\n",
    "largest_2006_4 = tabula.read_pdf('largest_2006.pdf', pages=4, area=[64, 90, 1005, 633])\n",
    "raw_fire_array.append(largest_2006_4)\n",
    "#2007 3 pages\n",
    "largest_2007_1 = tabula.read_pdf('largest_2007.pdf', pages=1, area=[97, 86, 973, 793])\n",
    "raw_fire_array.append(largest_2007_1)\n",
    "largest_2007_2 = tabula.read_pdf('largest_2007.pdf', pages=2, area=[97, 86, 973, 793])\n",
    "raw_fire_array.append(largest_2007_2)\n",
    "largest_2007_3 = tabula.read_pdf('largest_2007.pdf', pages=3, area=[97, 86, 973, 634])\n",
    "raw_fire_array.append(largest_2007_3)\n",
    "#2008 6 pages\n",
    "largest_2008_1 = tabula.read_pdf('largest_2008.pdf', pages=1, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_1)\n",
    "largest_2008_2 = tabula.read_pdf('largest_2008.pdf', pages=2, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_2)\n",
    "largest_2008_3 = tabula.read_pdf('largest_2008.pdf', pages=3, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_3)\n",
    "largest_2008_4 = tabula.read_pdf('largest_2008.pdf', pages=4, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_4)\n",
    "largest_2008_5 = tabula.read_pdf('largest_2008.pdf', pages=5, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_5)\n",
    "largest_2008_6 = tabula.read_pdf('largest_2008.pdf', pages=1, area=[69, 86, 1034, 260])\n",
    "raw_fire_array.append(largest_2008_6)\n",
    "#2009 3 pages\n",
    "largest_2009_1 = tabula.read_pdf('largest_2009.pdf', pages=1, area=[72, 199, 1052, 780])\n",
    "raw_fire_array.append(largest_2009_1)\n",
    "largest_2009_2 = tabula.read_pdf('largest_2009.pdf', pages=2, area=[72, 89, 1052, 788])\n",
    "raw_fire_array.append(largest_2009_2)\n",
    "largest_2009_3 = tabula.read_pdf('largest_2009.pdf', pages=3, area=[72, 89, 1052, 390])\n",
    "raw_fire_array.append(largest_2009_3)\n",
    "#2010 2 pages\n",
    "largest_2010_1 = tabula.read_pdf('largest_2010.pdf', pages=1, area=[69, 435, 1052, 780])\n",
    "raw_fire_array.append(largest_2010_1)\n",
    "largest_2010_2 = tabula.read_pdf('largest_2010.pdf', pages=2, area=[69, 88, 1052, 738])\n",
    "raw_fire_array.append(largest_2010_2)\n",
    "#2011 2 pages\n",
    "largest_2011_1 = tabula.read_pdf('largest_2011.pdf', pages=1, area=[86, 117, 1037, 815])\n",
    "raw_fire_array.append(largest_2011_1)\n",
    "largest_2011_2 = tabula.read_pdf('largest_2011.pdf', pages=2, area=[86, 121, 1037, 731])\n",
    "raw_fire_array.append(largest_2011_2)\n",
    "#2013 2 pages (remember 2012 link is broken)\n",
    "largest_2013_1 = tabula.read_pdf('largest_2013.pdf', pages=1, area=[102, 90, 960, 785])\n",
    "raw_fire_array.append(largest_2013_1)\n",
    "largest_2013_2 = tabula.read_pdf('largest_2013.pdf', pages=2, area=[102, 50, 960, 417])\n",
    "raw_fire_array.append(largest_2013_2)\n",
    "#2014 2 pages\n",
    "largest_2014_1 = tabula.read_pdf('largest_2014.pdf', pages=1, area=[220, 155, 1658, 1332])\n",
    "raw_fire_array.append(largest_2014_1)\n",
    "largest_2014_2 = tabula.read_pdf('largest_2014.pdf', pages=2, area=[220, 155, 1658, 818])\n",
    "raw_fire_array.append(largest_2014_2)\n",
    "#2015 3 pages\n",
    "largest_2015_1 = tabula.read_pdf('largest_2015.pdf', pages=1, area=[43, 93, 1058, 717])\n",
    "raw_fire_array.append(largest_2015_1)\n",
    "largest_2015_2 = tabula.read_pdf('largest_2015.pdf', pages=2, area=[43, 125, 1058, 670])\n",
    "raw_fire_array.append(largest_2015_2)\n",
    "largest_2015_3 = tabula.read_pdf('largest_2015.pdf', pages=3, area=[43, 107, 1058, 346])\n",
    "raw_fire_array.append(largest_2015_3)\n",
    "#2016 3 pages\n",
    "largest_2016_1 = tabula.read_pdf('largest_2016.pdf', pages=1, area=[38, 162, 1061, 792])\n",
    "raw_fire_array.append(largest_2016_1)\n",
    "largest_2016_2 = tabula.read_pdf('largest_2016.pdf', pages=2, area=[38, 177, 1061, 786])\n",
    "raw_fire_array.append(largest_2016_2)\n",
    "largest_2016_3 = tabula.read_pdf('largest_2016.pdf', pages=1, area=[38, 100, 1061, 372])\n",
    "raw_fire_array.append(largest_2016_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELP ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVERSAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_N = ['INCIDENT #', 'COUNTY/UNIT', 'FIRE_NAME', 'START_DATE', \n",
    "          'CONT_DATE', 'ORIGIN_DPT', 'BURNED_TOTAL', 'VEG_TYPE', \n",
    "          'CAUSE', 'STRUCT_DEST', 'STRUCT_DAM', 'FATALITY_FIRE', \n",
    "          'FATALITY_CIVIL']\n",
    "STRUCT_ROW = ['STRUCT_DEST', 'STRUCT_DAM']\n",
    "FATAL_ROW = ['FATALITY_FIRE', 'FATALITY_CIVIL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "column_separator(df, col, new_names)\n",
    "\n",
    "Separates one column into two (if values are separated by spaces)\n",
    "NOTE: this only works for fatality and structural columns\n",
    "\n",
    "params:\n",
    "df : DataFrame to change\n",
    "col : column name to split\n",
    "new_names : array of 2 new names for columns\n",
    "\n",
    "returns:\n",
    "pandas DataFrame with changes applied\n",
    "\"\"\"\n",
    "def column_separator(df, col, new_names):\n",
    "    column = df[col].astype(str).apply(lambda x : x.split(' '))\n",
    "    left_arr = []\n",
    "    right_arr = []\n",
    "    #uncomment below to see actual array values\n",
    "    #display(column)\n",
    "    \n",
    "    #iterate through values to separate them\n",
    "    for val in column:\n",
    "        #edge case array is len 1\n",
    "        if len(val) == 1:\n",
    "            left_arr.append(np.nan)\n",
    "            right_arr.append(np.nan)\n",
    "        #normal case array len 2\n",
    "        elif len(val) == 2:\n",
    "            left_arr.append(val[0])\n",
    "            right_arr.append(val[1])\n",
    "        #edge case array is bigger than expected\n",
    "        else:\n",
    "            left_arr.append('error')\n",
    "            right_arr.append('error')\n",
    "    #now make new column names and remove the old one\n",
    "    #display(left_arr)\n",
    "    #display(right_arr)\n",
    "    ans_df = df.drop(col, axis=1)\n",
    "    ans_df[new_names[0]] = left_arr\n",
    "    ans_df[new_names[1]] = right_arr\n",
    "    return ans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Datasets\n",
    "\n",
    "Mostly cleaned datasets anyways, largest issue is the combination of `County/Unit` and `Fire Name`. Normally, I would fix these issues by splitting on spaces (' ') but some departments and fire names contain more than one word so this method will not work. There are two ideas I have to solve this issues but it requires to add all the datasets in first\n",
    "\n",
    "This issue does not happen with all years which means some will have correct separation between `County/Unit` and `Fire Name` which means we can do one of two:\n",
    "1. Once we have all datasets combined, we can make a list of all counties/units and search for them in each grouped string\n",
    "2. There seems to be some correlation between the first three letters of Incident # and the county/unit, we can map the county from the Incident # (Incident # is fairly complete and easy to extract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2000\n",
    "#page 1\n",
    "#columns are rows, add correct columns, superfluous col\n",
    "l_2000_1 = largest_2000_1.append(largest_2000_1.columns.to_series(), ignore_index=True)\n",
    "l_2000_1 = l_2000_1.drop(['100%', 'Unnamed: 7', '400'], axis=1)\n",
    "l_2000_1.columns = ROWS_N\n",
    "#page 2\n",
    "#columns are a row, add correct cols, structural cols connected\n",
    "l_2000_2 = largest_2000_2.dropna(subset=['08/27/00'])\n",
    "l_2000_2 = l_2000_2.drop(['Unnamed: 13', 'Unnamed: 7', '8,084'], axis=1)\n",
    "l_2000_2 = l_2000_2.append(l_2000_2.columns.to_series(), ignore_index=True)\n",
    "l_2000_2['x'] = np.nan\n",
    "l_2000_2['y'] = np.nan\n",
    "l_2000_2.columns = ROWS_N\n",
    "l_2000_2['INCIDENT #'] = l_2000_2['INCIDENT #'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "#2001\n",
    "#page 1\n",
    "# columns and row 0 useless, col 11 useless\n",
    "# structure and fatality rows are stuck\n",
    "# drop acres burned broken (except total)\n",
    "l_2001_1 = largest_2001_1.drop(0).drop(['Unnamed: 11', '%', 'ACRES BURNED'], axis=1)\n",
    "l_2001_1 = column_separator(l_2001_1, 'STRUCTURES', STRUCT_ROW)\n",
    "l_2001_1 = column_separator(l_2001_1, 'FATALITIES', FATAL_ROW)\n",
    "l_2001_1.columns = ROWS_N\n",
    "l_2001_1\n",
    "# page 2\n",
    "#columns and row 0 useless, % col 3, 13 useless\n",
    "#drop nan start date\n",
    "#connected columns : unit/name structures fatalities\n",
    "# TODO: UNIT AND FIRE NAME\n",
    "l_2001_2 = largest_2001_2.drop(0).drop(['%', 'Unnamed: 3', 'ACRES BURNED', 'Unnamed: 13'], axis = 1)\n",
    "l_2001_2 = l_2001_2.dropna(subset=['DATE'])\n",
    "l_2001_2.insert(2, 'placeholder', -1)\n",
    "l_2001_2 = column_separator(l_2001_2, 'STRUCTURES', STRUCT_ROW)\n",
    "l_2001_2 = column_separator(l_2001_2, 'FATALITIES', FATAL_ROW)\n",
    "l_2001_2.columns = ROWS_N\n",
    "l_2001_2\n",
    "\n",
    "#2002\n",
    "#page 1\n",
    "#column is row\n",
    "#connected columns: unit/name\n",
    "# TODO: UNIT AND FIRE NAME\n",
    "l_2002_1 = largest_2002_1.drop(['100%', '320', 'Unnamed: 8'], axis=1)\n",
    "l_2002_1 = l_2002_1.append(l_2002_1.columns.to_series(), ignore_index=True)\n",
    "l_2002_1.columns = ROWS_N\n",
    "l_2002_1\n",
    "#page 2\n",
    "#column is row\n",
    "l_2002_2 = largest_2002_2.drop(['100%', 'Unnamed: 7', '670'], axis=1)\n",
    "l_2002_2 = l_2002_2.append(l_2002_2.columns.to_series(), ignore_index=True)\n",
    "l_2002_2.columns = ROWS_N\n",
    "l_2002_2\n",
    "#page 3\n",
    "#column is row\n",
    "#drop na dates\n",
    "#combined columns: name/date structure fatality\n",
    "l_2002_3 = largest_2002_3.drop(['Unnamed: 6', '1,200'], axis=1)\n",
    "l_2002_3 = column_separator(l_2002_3, 'Unnamed: 11', STRUCT_ROW)\n",
    "l_2002_3 = column_separator(l_2002_3, 'Unnamed: 12', FATAL_ROW)\n",
    "l_2002_3 = l_2002_3.dropna(subset=['Mountain 09/09/02'])\n",
    "l_2002_3.columns = ROWS_N\n",
    "#procedure for separating firename and date\n",
    "l_2002_3.START_DATE = l_2002_3.FIRE_NAME.apply(lambda x: x.split(' ')[-1])\n",
    "l_2002_3.FIRE_NAME = l_2002_3.FIRE_NAME.apply(lambda x: ' '.join(x.split(' ')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INCIDENT #</th>\n",
       "      <th>COUNTY/UNIT</th>\n",
       "      <th>FIRE_NAME</th>\n",
       "      <th>START_DATE</th>\n",
       "      <th>CONT_DATE</th>\n",
       "      <th>ORIGIN_DPT</th>\n",
       "      <th>BURNED_TOTAL</th>\n",
       "      <th>VEG_TYPE</th>\n",
       "      <th>CAUSE</th>\n",
       "      <th>STRUCT_DEST</th>\n",
       "      <th>STRUCT_DAM</th>\n",
       "      <th>FATALITY_FIRE</th>\n",
       "      <th>FATALITY_CIVIL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TUU-5954</td>\n",
       "      <td>Tulare</td>\n",
       "      <td>Richgrove</td>\n",
       "      <td>06/28/00</td>\n",
       "      <td>06/28/00</td>\n",
       "      <td>CDF</td>\n",
       "      <td>450</td>\n",
       "      <td>G</td>\n",
       "      <td>Elec. Power</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVU-4431</td>\n",
       "      <td>San Diego (exp002 in)</td>\n",
       "      <td>Barrett</td>\n",
       "      <td>06/29/00</td>\n",
       "      <td>06/30/00</td>\n",
       "      <td>CDF</td>\n",
       "      <td>2,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Under Invest.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MMU-9447</td>\n",
       "      <td>Mariposa</td>\n",
       "      <td>Granite</td>\n",
       "      <td>07/02/00</td>\n",
       "      <td>07/04/00</td>\n",
       "      <td>CDF</td>\n",
       "      <td>2,000</td>\n",
       "      <td>WB</td>\n",
       "      <td>Shooter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BDU-5166</td>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>Citrus</td>\n",
       "      <td>07/03/00</td>\n",
       "      <td>07/04/00</td>\n",
       "      <td>CDF</td>\n",
       "      <td>600</td>\n",
       "      <td>G</td>\n",
       "      <td>Under Invest.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BDU-5318</td>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>Assist (Morgan)</td>\n",
       "      <td>07/07/00</td>\n",
       "      <td>07/08/00</td>\n",
       "      <td>USFS</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Under Invest.</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  INCIDENT #            COUNTY/UNIT        FIRE_NAME START_DATE CONT_DATE  \\\n",
       "0   TUU-5954                 Tulare        Richgrove   06/28/00  06/28/00   \n",
       "1   MVU-4431  San Diego (exp002 in)          Barrett   06/29/00  06/30/00   \n",
       "2   MMU-9447               Mariposa          Granite   07/02/00  07/04/00   \n",
       "3   BDU-5166         San Bernardino           Citrus   07/03/00  07/04/00   \n",
       "4   BDU-5318         San Bernardino  Assist (Morgan)   07/07/00  07/08/00   \n",
       "\n",
       "  ORIGIN_DPT BURNED_TOTAL VEG_TYPE          CAUSE STRUCT_DEST STRUCT_DAM  \\\n",
       "0        CDF          450        G    Elec. Power           0          0   \n",
       "1        CDF        2,000      NaN  Under Invest.         NaN        NaN   \n",
       "2        CDF        2,000       WB        Shooter           0          0   \n",
       "3        CDF          600        G  Under Invest.           0          0   \n",
       "4       USFS          300      NaN  Under Invest.          na         na   \n",
       "\n",
       "  FATALITY_FIRE FATALITY_CIVIL  \n",
       "0             0              0  \n",
       "1           NaN            NaN  \n",
       "2             0              0  \n",
       "3             0              0  \n",
       "4            na             na  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined dataset, this will grow as more datasets are cleaned\n",
    "clean_df = pd.concat([l_2000_1, l_2000_2,\n",
    "                      l_2001_1, l_2001_2,\n",
    "                      l_2002_1, l_2002_2, l_2002_3], ignore_index=True, sort=False)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Sections\n",
    "\n",
    "While not data sets are added (yet!), the coding for other sections can start since these are the final columns kept, as more data is added, the code should mould to the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
