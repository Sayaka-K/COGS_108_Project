{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there will be more packages added as needed\n",
    "#standard datascience packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#packages for reading geojson file\n",
    "import json\n",
    "import os\n",
    "#package for reading sql lite database\n",
    "import sqlite3\n",
    "#package for map making\n",
    "import folium\n",
    "#package for accurate date/time manipulations\n",
    "import datetime\n",
    "#note this package is NOT native to Anaconda, follow tabula\n",
    "#documentation for steps on installation\n",
    "#package for PDF scraping\n",
    "import tabula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEOJSON\n",
    "\n",
    "This GeoJSON file contains the boundries for all California Counties as drawn by CALFIRE's Fire and Resource Assessment Program (FRAP). This dataset will help in determining boundries for map vizualizations.\n",
    "\n",
    "**Data Source:** https://data.ca.gov/dataset/california-counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OBJECTID': 1,\n",
       " 'COUNTY_NAME': 'Alameda County',\n",
       " 'COUNTY_ABBREV': 'ALA',\n",
       " 'COUNTY_NUM': 1,\n",
       " 'COUNTY_CODE': '01',\n",
       " 'COUNTY_FIPS': '001'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_counties = json.load(open('California_Counties.geojson'))\n",
    "ca_counties['features'][0]['properties']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Database\n",
    "\n",
    "SQL Lite database obtained from Kaggle contains 1.88 million wildfires throughout the US and US Territories. For the purposes of our analysis we will only use fires that are located in California. This dataset contains a myriad of information such as different ID types, latitude and longitude measurements, fire size, discovery date, and containment date. This dataset will be used in conjunction with the GeoJSON file to make the map vizualizations\n",
    "\n",
    "**Data Source:** https://www.kaggle.com/rtatman/188-million-us-wildfires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM fires': no such table: fires",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: fires",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a182ca9dbe7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fire_sql.sqlite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM fires\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfires\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    312\u001b[0m     return pandas_sql.read_query(\n\u001b[1;32m    313\u001b[0m         \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1388\u001b[0m             ex = DatabaseError(\n\u001b[1;32m   1389\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[0;34m(exc, traceback)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;31m# this version of raise is a syntax error in Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM fires': no such table: fires"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"fire_sql.sqlite\")\n",
    "fires = pd.read_sql_query(\"SELECT * FROM fires\", conn)\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF SCRAPING\n",
    "\n",
    "Unfortunately there is not any good datasets for wildfires that are in a nice format. For this reason, we have decided to collect the data ourselves the messy way from... PDF files. The PDF format was not meant to be read by a computer but by using the `Tabula` package, an open source PDF scraper, we are able to read pdf tables directly to pandas DataFrames. The tool was far from perfect, combined with inconsistencies in government data collection, required each pdf to be scraped individually page by page. To further complicate things, the DataFrames read are very messy. Data cleaning/data wrangling will be covered in the next section.\n",
    "\n",
    "**Link to example pdf:** http://cdfdata.fire.ca.gov/pub/cdf/images/incidentstatsevents_272.pdf\n",
    "\n",
    "**Data Source:** http://cdfdata.fire.ca.gov/incidents/incidents_statsevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will scrape some data directly from pdf using tabula open source pdf reader.\n",
    "\n",
    "#variable format largest_[year]_[page]\n",
    "#2012 link is broken\n",
    "# (do this later) TODO: drop anything that is missing a date\n",
    "raw_fire_array = []\n",
    "#2000 2 pages\n",
    "largest_2000_1 = tabula.read_pdf('largest_2000.pdf', pages=1, area=[60, 105, 1004, 800])\n",
    "raw_fire_array.append(largest_2000_1)\n",
    "largest_2000_2 = tabula.read_pdf('largest_2000.pdf', pages=2, area=[60, 51, 1004, 650])\n",
    "raw_fire_array.append(largest_2000_2)\n",
    "#2001 2 pages\n",
    "largest_2001_1 = tabula.read_pdf('largest_2001.pdf', pages=1, area=[100, 88, 952, 792])\n",
    "raw_fire_array.append(largest_2001_1)\n",
    "largest_2001_2 = tabula.read_pdf('largest_2001.pdf', pages=2, area=[100, 46, 952, 623])\n",
    "raw_fire_array.append(largest_2001_2)\n",
    "#2002 3 pages\n",
    "largest_2002_1 = tabula.read_pdf('largest_2002.pdf', pages=1, area=[76, 104, 1010, 797])\n",
    "raw_fire_array.append(largest_2002_1)\n",
    "largest_2002_2 = tabula.read_pdf('largest_2002.pdf', pages=2, area=[76, 50, 1010, 786])\n",
    "raw_fire_array.append(largest_2002_2)\n",
    "largest_2002_3 = tabula.read_pdf('largest_2002.pdf', pages=3, area=[76, 50, 1010, 681])\n",
    "raw_fire_array.append(largest_2002_3)\n",
    "#2003 3 pages\n",
    "largest_2003_1 = tabula.read_pdf('largest_2003.pdf', pages=1, area=[58, 81, 1015, 787])\n",
    "raw_fire_array.append(largest_2003_1)\n",
    "largest_2003_2 = tabula.read_pdf('largest_2003.pdf', pages=2, area=[58, 33, 1015, 784])\n",
    "raw_fire_array.append(largest_2003_2)\n",
    "largest_2003_3 = tabula.read_pdf('largest_2003.pdf', pages=3, area=[58, 33, 1015, 169])\n",
    "raw_fire_array.append(largest_2003_3)\n",
    "#2004 2 pages\n",
    "largest_2004_1 = tabula.read_pdf('largest_2004.pdf', pages=1, area=[81, 125, 1017, 797])\n",
    "raw_fire_array.append(largest_2004_1)\n",
    "largest_2004_2 = tabula.read_pdf('largest_2004.pdf', pages=2, area=[81, 76, 1017, 646])\n",
    "raw_fire_array.append(largest_2004_2)\n",
    "#2005 3 pages\n",
    "largest_2005_1 = tabula.read_pdf('largest_2005.pdf', pages=1, area=[110, 86, 958, 793])\n",
    "raw_fire_array.append(largest_2005_1)\n",
    "largest_2005_2 = tabula.read_pdf('largest_2005.pdf', pages=2, area=[110, 86, 958, 793])\n",
    "raw_fire_array.append(largest_2005_2)\n",
    "largest_2005_3 = tabula.read_pdf('largest_2005.pdf', pages=3, area=[110, 86, 958, 385])\n",
    "raw_fire_array.append(largest_2005_3)\n",
    "#2006 4 pages\n",
    "largest_2006_1 = tabula.read_pdf('largest_2006.pdf', pages=1, area=[64, 90, 1005, 806])\n",
    "raw_fire_array.append(largest_2006_1)\n",
    "largest_2006_2 = tabula.read_pdf('largest_2006.pdf', pages=2, area=[64, 90, 1005, 806])\n",
    "raw_fire_array.append(largest_2006_2)\n",
    "largest_2006_3 = tabula.read_pdf('largest_2006.pdf', pages=3, area=[64, 90, 1005, 806])\n",
    "raw_fire_array.append(largest_2006_3)\n",
    "largest_2006_4 = tabula.read_pdf('largest_2006.pdf', pages=4, area=[64, 90, 1005, 633])\n",
    "raw_fire_array.append(largest_2006_4)\n",
    "#2007 3 pages\n",
    "largest_2007_1 = tabula.read_pdf('largest_2007.pdf', pages=1, area=[97, 86, 973, 793])\n",
    "raw_fire_array.append(largest_2007_1)\n",
    "largest_2007_2 = tabula.read_pdf('largest_2007.pdf', pages=2, area=[97, 86, 973, 793])\n",
    "raw_fire_array.append(largest_2007_2)\n",
    "largest_2007_3 = tabula.read_pdf('largest_2007.pdf', pages=3, area=[97, 86, 973, 634])\n",
    "raw_fire_array.append(largest_2007_3)\n",
    "#2008 6 pages\n",
    "largest_2008_1 = tabula.read_pdf('largest_2008.pdf', pages=1, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_1)\n",
    "largest_2008_2 = tabula.read_pdf('largest_2008.pdf', pages=2, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_2)\n",
    "largest_2008_3 = tabula.read_pdf('largest_2008.pdf', pages=3, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_3)\n",
    "largest_2008_4 = tabula.read_pdf('largest_2008.pdf', pages=4, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_4)\n",
    "largest_2008_5 = tabula.read_pdf('largest_2008.pdf', pages=5, area=[69, 86, 1034, 804])\n",
    "raw_fire_array.append(largest_2008_5)\n",
    "largest_2008_6 = tabula.read_pdf('largest_2008.pdf', pages=1, area=[69, 86, 1034, 260])\n",
    "raw_fire_array.append(largest_2008_6)\n",
    "#2009 3 pages\n",
    "largest_2009_1 = tabula.read_pdf('largest_2009.pdf', pages=1, area=[72, 199, 1052, 780])\n",
    "raw_fire_array.append(largest_2009_1)\n",
    "largest_2009_2 = tabula.read_pdf('largest_2009.pdf', pages=2, area=[72, 89, 1052, 788])\n",
    "raw_fire_array.append(largest_2009_2)\n",
    "largest_2009_3 = tabula.read_pdf('largest_2009.pdf', pages=3, area=[72, 89, 1052, 390])\n",
    "raw_fire_array.append(largest_2009_3)\n",
    "#2010 2 pages\n",
    "largest_2010_1 = tabula.read_pdf('largest_2010.pdf', pages=1, area=[69, 435, 1052, 780])\n",
    "raw_fire_array.append(largest_2010_1)\n",
    "largest_2010_2 = tabula.read_pdf('largest_2010.pdf', pages=2, area=[69, 88, 1052, 738])\n",
    "raw_fire_array.append(largest_2010_2)\n",
    "#2011 2 pages\n",
    "largest_2011_1 = tabula.read_pdf('largest_2011.pdf', pages=1, area=[86, 117, 1037, 815])\n",
    "raw_fire_array.append(largest_2011_1)\n",
    "largest_2011_2 = tabula.read_pdf('largest_2011.pdf', pages=2, area=[86, 121, 1037, 731])\n",
    "raw_fire_array.append(largest_2011_2)\n",
    "#2013 2 pages (remember 2012 link is broken)\n",
    "largest_2013_1 = tabula.read_pdf('largest_2013.pdf', pages=1, area=[102, 90, 960, 785])\n",
    "raw_fire_array.append(largest_2013_1)\n",
    "largest_2013_2 = tabula.read_pdf('largest_2013.pdf', pages=2, area=[102, 50, 960, 417])\n",
    "raw_fire_array.append(largest_2013_2)\n",
    "#2014 2 pages\n",
    "largest_2014_1 = tabula.read_pdf('largest_2014.pdf', pages=1, area=[220, 155, 1658, 1332])\n",
    "raw_fire_array.append(largest_2014_1)\n",
    "largest_2014_2 = tabula.read_pdf('largest_2014.pdf', pages=2, area=[220, 155, 1658, 818])\n",
    "raw_fire_array.append(largest_2014_2)\n",
    "#2015 3 pages\n",
    "largest_2015_1 = tabula.read_pdf('largest_2015.pdf', pages=1, area=[43, 93, 1058, 717])\n",
    "raw_fire_array.append(largest_2015_1)\n",
    "largest_2015_2 = tabula.read_pdf('largest_2015.pdf', pages=2, area=[43, 125, 1058, 670])\n",
    "raw_fire_array.append(largest_2015_2)\n",
    "largest_2015_3 = tabula.read_pdf('largest_2015.pdf', pages=3, area=[43, 107, 1058, 346])\n",
    "raw_fire_array.append(largest_2015_3)\n",
    "#2016 3 pages\n",
    "largest_2016_1 = tabula.read_pdf('largest_2016.pdf', pages=1, area=[38, 162, 1061, 792])\n",
    "raw_fire_array.append(largest_2016_1)\n",
    "largest_2016_2 = tabula.read_pdf('largest_2016.pdf', pages=2, area=[38, 177, 1061, 786])\n",
    "raw_fire_array.append(largest_2016_2)\n",
    "largest_2016_3 = tabula.read_pdf('largest_2016.pdf', pages=3, area=[38, 100, 1061, 372])\n",
    "raw_fire_array.append(largest_2016_3)\n",
    "\n",
    "#45 total pages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messy DataFrame Example\n",
    "\n",
    "The table is practically unreadable, many problems to fix such as combined columns, NaN/missing columns, useless rows, and incorrect/missing column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>DATE</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>ACRES BURNED</th>\n",
       "      <th>VEG.</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>STRUCTURESFATALITIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INC. # COUNTY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FIRE NAME</td>\n",
       "      <td>START CONT. DPA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CDF OTHER TOTAL</td>\n",
       "      <td>TYPE</td>\n",
       "      <td>CAUSE</td>\n",
       "      <td>DEST. DAM. FIRE CIVIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BDF-3917 SAN BERNARDINO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UPPER SANTA ANA</td>\n",
       "      <td>03/25/04 03/26/04 USFS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350 350</td>\n",
       "      <td>BT</td>\n",
       "      <td>ESCAPE</td>\n",
       "      <td>1 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHF-000443 TEHAMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHF ASSIST (HAMILTON)</td>\n",
       "      <td>04/09/04 04/12/04 USFS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301 301</td>\n",
       "      <td>T</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RRU-032913 RIVERSIDE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PLEASURE FIRE</td>\n",
       "      <td>04/25/04 04/26/04 CDF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,464 2,464</td>\n",
       "      <td>GB</td>\n",
       "      <td>VEHICLE</td>\n",
       "      <td>9 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MVU-003337 SAN DIEGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASSIST #31 (INDIA FIRE)</td>\n",
       "      <td>05/02/04 05/03/04 MIL.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,040 2,040</td>\n",
       "      <td>BT</td>\n",
       "      <td>UI</td>\n",
       "      <td>0 0 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unnamed: 0  Unnamed: 1               Unnamed: 2  \\\n",
       "0            INC. # COUNTY         NaN                FIRE NAME   \n",
       "1  BDF-3917 SAN BERNARDINO         NaN          UPPER SANTA ANA   \n",
       "2        SHF-000443 TEHAMA         NaN    SHF ASSIST (HAMILTON)   \n",
       "3     RRU-032913 RIVERSIDE         NaN            PLEASURE FIRE   \n",
       "4     MVU-003337 SAN DIEGO         NaN  ASSIST #31 (INDIA FIRE)   \n",
       "\n",
       "                     DATE  Unnamed: 4     ACRES BURNED  VEG. Unnamed: 7  \\\n",
       "0         START CONT. DPA         NaN  CDF OTHER TOTAL  TYPE      CAUSE   \n",
       "1  03/25/04 03/26/04 USFS         NaN          350 350    BT     ESCAPE   \n",
       "2  04/09/04 04/12/04 USFS         NaN          301 301     T      HUMAN   \n",
       "3   04/25/04 04/26/04 CDF         NaN      2,464 2,464    GB    VEHICLE   \n",
       "4  05/02/04 05/03/04 MIL.         NaN      2,040 2,040    BT         UI   \n",
       "\n",
       "    STRUCTURESFATALITIES  \n",
       "0  DEST. DAM. FIRE CIVIL  \n",
       "1                1 0 0 0  \n",
       "2                0 0 0 0  \n",
       "3                9 0 0 0  \n",
       "4                0 0 0 0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "largest_2004_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIBLE NEW AVENUES\n",
    "National overall statistics on fires and economic losses \n",
    "\n",
    "https://www.usfa.fema.gov/data/statistics/ \n",
    "\n",
    "\n",
    "Link with federal government resources like firestations (most data are restricted)\n",
    "\n",
    "https://hifld-geoplatform.opendata.arcgis.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVERSAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row names for cleaned tables\n",
    "ROWS_N = ['INCIDENT #', 'COUNTY/UNIT', 'FIRE_NAME', 'START_DATE', \n",
    "          'CONT_DATE', 'ORIGIN_DPT', 'BURNED_TOTAL', 'VEG_TYPE', \n",
    "          'CAUSE', 'STRUCT_DEST', 'STRUCT_DAM', 'FATALITY_FIRE', \n",
    "          'FATALITY_CIVIL']\n",
    "STRUCT_ROW = ['STRUCT_DEST', 'STRUCT_DAM']\n",
    "FATAL_ROW = ['FATALITY_FIRE', 'FATALITY_CIVIL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fix 2000 - 2002 Years to use robust_col_sep\n",
    "\"\"\"\n",
    "NOTE: this only works for fatality and structural columns\n",
    "\n",
    "column_separator(df, col, new_names)\n",
    "\n",
    "Separates one column into two (if values are separated by spaces)\n",
    "\n",
    "params:\n",
    "df : DataFrame to change\n",
    "col : column name to split\n",
    "new_names : array of 2 new names for columns\n",
    "\n",
    "returns:\n",
    "pandas DataFrame with changes applied\n",
    "\"\"\"\n",
    "def column_separator(df, col, new_names):\n",
    "    column = df[col].astype(str).apply(lambda x : x.split(' '))\n",
    "    left_arr = []\n",
    "    right_arr = []\n",
    "    #uncomment below to see actual array values\n",
    "    #display(column)\n",
    "    \n",
    "    #iterate through values to separate them\n",
    "    for val in column:\n",
    "        #edge case array is len 1\n",
    "        if len(val) == 1:\n",
    "            left_arr.append(np.nan)\n",
    "            right_arr.append(np.nan)\n",
    "        #normal case array len 2\n",
    "        elif len(val) == 2:\n",
    "            left_arr.append(val[0])\n",
    "            right_arr.append(val[1])\n",
    "        #edge case array is bigger than expected\n",
    "        else:\n",
    "            left_arr.append('error')\n",
    "            right_arr.append('error')\n",
    "    #now make new column names and remove the old one\n",
    "    #display(left_arr)\n",
    "    #display(right_arr)\n",
    "    ans_df = df.drop(col, axis=1)\n",
    "    ans_df[new_names[0]] = left_arr\n",
    "    ans_df[new_names[1]] = right_arr\n",
    "    return ans_df\n",
    "\"\"\"\n",
    "robust_col_sep(df, iloc, new_names, front=True)\n",
    "\n",
    "Separates one column into two columns, values must be separated by spaces; drops the original column at iloc index\n",
    "\n",
    "params:\n",
    "\n",
    "df : the dataframe to modify\n",
    "index : the index of the column as given by iloc indexing (int)\n",
    "new_names : the new names for the two new columns created (array size 2)\n",
    "front : determines whether to separate the value from the front or from the back (boolean, optional):\n",
    "\n",
    "Example:\n",
    "Consider the following string:\n",
    "Apple Cinnamon Bannana\n",
    "\n",
    "front = True will give\n",
    "[Apple] [Cinnamon Bannana]\n",
    "\n",
    "front = False will give\n",
    "[Apple Cinnamon] [Bannana]\n",
    "\n",
    "returns:\n",
    "pandas DataFrame with changes applied\n",
    "\"\"\"\n",
    "\n",
    "def robust_col_sep(df, index, new_names, front=True):\n",
    "    column = df.iloc[:, index].astype(str).apply(lambda x : x.split(' '))\n",
    "    left_arr = []\n",
    "    right_arr = []\n",
    "    \n",
    "    #display(column)\n",
    "    #iterate through column vals\n",
    "    for val in column:\n",
    "        if front:\n",
    "            left_arr.append(val[0])\n",
    "            right_arr.append(' '.join(val[1:]))\n",
    "        #if not front\n",
    "        else:\n",
    "            left_arr.append(' '.join(val[:-1]))\n",
    "            right_arr.append(val[-1])\n",
    "    #make new cols and drop old\n",
    "    ans_df = df.drop(df.columns[index], axis=1)\n",
    "    #add right column first\n",
    "    #display(len(left_arr))\n",
    "    #display(len(right_arr))\n",
    "    #display(len(df.index))\n",
    "    ans_df.insert(index, new_names[1], right_arr)\n",
    "    ans_df.insert(index, new_names[0], left_arr)\n",
    "    return ans_df\n",
    "\n",
    "############################################################################\n",
    "#                           APPLY HELPER FUNCTIONS                         #\n",
    "############################################################################\n",
    "\n",
    "'''\n",
    "APPLY HELPER FUNCTION (used in Series.apply() only)\n",
    "\n",
    "Takes in an array with year and DOY (Day Of Year) and converts it to a datetime object.\n",
    "This function is needed due to odd dating format in SQL table\n",
    "'''\n",
    "def datetime_convert_sql(year_day_array):\n",
    "    year = int(year_day_array[0])\n",
    "    #issue with nan values not converting to int values\n",
    "    try:\n",
    "        day = int(year_day_array[1].split('.')[0])\n",
    "        ans = datetime.datetime(year, 1, 1) + datetime.timedelta(days=day-1)\n",
    "    except ValueError:\n",
    "        ans = np.nan\n",
    "    return ans\n",
    "'''\n",
    "APPLY HELPER FUNCTION (used in Series.apply() only)\n",
    "\n",
    "Takes in a string in the format MM/DD/YY and converts it to a datetime object\n",
    "This function is needed due to YY format and NaN error handling\n",
    "'''\n",
    "def datetime_convert_pdf(date):\n",
    "    date_arr = date.split('/')\n",
    "    if len(date_arr) == 3:\n",
    "        return datetime.datetime(int('20'+date_arr[2].split('.')[0]), int(date_arr[0]), int(date_arr[1]))\n",
    "    else:\n",
    "        return np.nan\n",
    "'''\n",
    "APPLY HELPER FUNCTION (used in Series.apply() only)\n",
    "\n",
    "Takes in a string value and converts to integer value\n",
    "This function is needed for NaN value handleing to join PDF table and SQL table\n",
    "'''\n",
    "def int_convert(val):\n",
    "    try:\n",
    "        ans = int(val)\n",
    "    except:\n",
    "        ans = -1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL TABLE CLEANING\n",
    "\n",
    "The SQL table is largely clean already, we will just drop a few extra columns and convert the strange dates into datetime objects as well as clean up the `FIRE_SIZE` column into `int` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fires' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2566e85a3321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TODO: CLEAN UP THIS SPAGHETTI CODE MAN!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfires\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfires\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfires\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfires\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m38\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#preparing for date conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfires\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'START_DATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfires\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FIRE_YEAR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DISCOVERY_DOY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fires' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: CLEAN UP THIS SPAGHETTI CODE MAN!\n",
    "fires = fires[fires['STATE'] == 'CA']\n",
    "fires = fires.drop(columns=fires.columns[[1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 14, 15, 16, 17, 18, 24, 25, 32, 33, 36, 37, 38]])\n",
    "#preparing for date conversion\n",
    "fires['START_DATE'] = fires[['FIRE_YEAR', 'DISCOVERY_DOY']].astype(str).apply(lambda x: ' '.join(x).split(' '), axis=1)\n",
    "fires['END_DATE'] = fires[['FIRE_YEAR', 'CONT_DOY']].astype(str).apply(lambda x: ' '.join(x).split(' '), axis=1)\n",
    "#date conversion\n",
    "fires['START_DATE'] = fires.START_DATE.apply(datetime_convert_sql)\n",
    "fires['END_DATE'] = fires.END_DATE.apply(datetime_convert_sql)\n",
    "fires = fires.drop(['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', \n",
    "                    'CONT_DOY'], axis=1)\n",
    "fires.FIRE_SIZE = fires.FIRE_SIZE.astype(int)\n",
    "fires.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF TABLE CLEANING\n",
    "\n",
    "Mostly cleaned datasets anyways, largest issue is the combination of `County/Unit` and `Fire Name`. Normally, I would fix these issues by splitting on spaces (' ') but some departments and fire names contain more than one word so this method will not work. There are two ideas I have to solve this issues but it requires to add all the datasets in first\n",
    "\n",
    "This issue does not happen with all years which means some will have correct separation between `County/Unit` and `Fire Name` which means we can do one of two:\n",
    "1. Once we have all datasets combined, we can make a list of all counties/units and search for them in each grouped string\n",
    "2. There seems to be some correlation between the first three letters of Incident # and the county/unit, we can map the county from the Incident # (Incident # is fairly complete and easy to extract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dirty 2016 page 2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ACRES BURNED</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>STRUCTURES</th>\n",
       "      <th>FATALITIES</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INCIDENT # COUNTY / UNIT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FIRE NAME</td>\n",
       "      <td>START CONT.</td>\n",
       "      <td>DPA</td>\n",
       "      <td>CAL FIRE OTHER</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>VEG. TYPE</td>\n",
       "      <td>CAUSE</td>\n",
       "      <td>DEST. DAM.</td>\n",
       "      <td>FIRE CIVIL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHER AGENCY INCIDENTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLU-5280 SAN LUIS OBISPO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAMP ROBERTS</td>\n",
       "      <td>05/18/16 05/20/16</td>\n",
       "      <td>MIL</td>\n",
       "      <td>3,712</td>\n",
       "      <td>3,712</td>\n",
       "      <td>G</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CND-1134 TULARE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHIMNEY</td>\n",
       "      <td>06/01/16 06/07/16</td>\n",
       "      <td>BLM</td>\n",
       "      <td>1,324</td>\n",
       "      <td>1,324</td>\n",
       "      <td>BG</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FHL-1475 MONTEREY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STONY</td>\n",
       "      <td>06/02/16 06/17/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>3,000</td>\n",
       "      <td>3,000</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CND-1171 SAN LUIS OBISPO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SODA</td>\n",
       "      <td>06/04/16 06/06/16</td>\n",
       "      <td>BLM</td>\n",
       "      <td>2,003</td>\n",
       "      <td>2,003</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LAC-457 LOS ANGELES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OLD</td>\n",
       "      <td>06/04/16 06/10/16</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>465</td>\n",
       "      <td>465</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>9 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LPF-1504 MONTEREY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLEMAN</td>\n",
       "      <td>06/04/16 06/17/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,520</td>\n",
       "      <td>2,520</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNF-4500 SISKIYOU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PONY</td>\n",
       "      <td>06/07/16 10/02/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,860</td>\n",
       "      <td>2,860</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SBC-7344 SANTA BARBARA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SHERPA</td>\n",
       "      <td>06/15/16 06/23/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,990 4,484</td>\n",
       "      <td>7,474</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANF-2417 LOS ANGELES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RESERVOIR</td>\n",
       "      <td>06/20/16 07/05/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>1,146</td>\n",
       "      <td>1,146</td>\n",
       "      <td>BG</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LAC-177288 LOS ANGELES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FISH</td>\n",
       "      <td>06/20/16 07/05/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>4,253</td>\n",
       "      <td>4,253</td>\n",
       "      <td>BG</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CND-1415 KERN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ERSKINE</td>\n",
       "      <td>06/23/16 07/09/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>48,019</td>\n",
       "      <td>48,019</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>286 12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INF-992 MONO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARINA</td>\n",
       "      <td>06/24/16 07/04/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>654</td>\n",
       "      <td>654</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>YNP-41 MARIPOSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LAKES</td>\n",
       "      <td>06/26/16 10/31/16</td>\n",
       "      <td>NPS</td>\n",
       "      <td>1,001</td>\n",
       "      <td>1,001</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LPF-1986 VENTURA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PINE</td>\n",
       "      <td>06/30/16 07/17/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,304</td>\n",
       "      <td>2,304</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KRN-24109 KERN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEER</td>\n",
       "      <td>07/01/16 07/10/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>1,785</td>\n",
       "      <td>1,785</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KRN-25390 KERN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FORT</td>\n",
       "      <td>07/08/16 07/09/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>554</td>\n",
       "      <td>554</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LAC-198015 LOS ANGELES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAGE</td>\n",
       "      <td>07/09/16 07/16/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>990 110</td>\n",
       "      <td>1,100</td>\n",
       "      <td>BGT</td>\n",
       "      <td>CAMPFIRE</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MCP-2125 SAN DIEGO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROBLAR</td>\n",
       "      <td>07/21/16 07/27/16</td>\n",
       "      <td>MIL</td>\n",
       "      <td>1,245</td>\n",
       "      <td>1,245</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ANF-3008 LOS ANGELES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAND</td>\n",
       "      <td>07/22/16 08/06/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,188 39,195</td>\n",
       "      <td>41,383</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>116 20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>INF-1415 MONO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLARK</td>\n",
       "      <td>08/04/16 08/12/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,819</td>\n",
       "      <td>2,819</td>\n",
       "      <td>BGT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BDF-10205 SAN BERNARDINO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PILOT</td>\n",
       "      <td>08/07/16 08/16/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>472 7,638</td>\n",
       "      <td>8,110</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>INF-1457 INYO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HORSESHOE</td>\n",
       "      <td>08/09/16 08/17/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SQF-2595 KERN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CEDAR</td>\n",
       "      <td>08/16/16 09/24/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>29,322</td>\n",
       "      <td>29,322</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BDF-10468 SAN BERNARDINO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BLUECUT</td>\n",
       "      <td>08/16/16 09/08/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>36,274</td>\n",
       "      <td>36,274</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>321 8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LPF-2809 SANTA BARBARA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REY</td>\n",
       "      <td>08/18/16 09/15/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>32,606</td>\n",
       "      <td>32,606</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>STF-2257 ALPINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOKELUMNE</td>\n",
       "      <td>08/19/16 09/20/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>655</td>\n",
       "      <td>655</td>\n",
       "      <td>BGT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SQF-2683 TULARE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TULE</td>\n",
       "      <td>08/22/16 08/22/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>395</td>\n",
       "      <td>395</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>KRN-32226 KERN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RANGE</td>\n",
       "      <td>08/26/16 08/31/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>BGT</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KNF-7501 SISKIYOU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAP</td>\n",
       "      <td>08/27/16 09/16/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>33,867</td>\n",
       "      <td>33,867</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Unnamed: 0  Unnamed: 1    Unnamed: 2               DATE  \\\n",
       "0   INCIDENT # COUNTY / UNIT         NaN     FIRE NAME        START CONT.   \n",
       "1     OTHER AGENCY INCIDENTS         NaN           NaN                NaN   \n",
       "2   SLU-5280 SAN LUIS OBISPO         NaN  CAMP ROBERTS  05/18/16 05/20/16   \n",
       "3            CND-1134 TULARE         NaN       CHIMNEY  06/01/16 06/07/16   \n",
       "4          FHL-1475 MONTEREY         NaN         STONY  06/02/16 06/17/16   \n",
       "5   CND-1171 SAN LUIS OBISPO         NaN          SODA  06/04/16 06/06/16   \n",
       "6        LAC-457 LOS ANGELES         NaN           OLD  06/04/16 06/10/16   \n",
       "7          LPF-1504 MONTEREY         NaN       COLEMAN  06/04/16 06/17/16   \n",
       "8          KNF-4500 SISKIYOU         NaN          PONY  06/07/16 10/02/16   \n",
       "9     SBC-7344 SANTA BARBARA         NaN        SHERPA  06/15/16 06/23/16   \n",
       "10      ANF-2417 LOS ANGELES         NaN     RESERVOIR  06/20/16 07/05/16   \n",
       "11    LAC-177288 LOS ANGELES         NaN          FISH  06/20/16 07/05/16   \n",
       "12             CND-1415 KERN         NaN       ERSKINE  06/23/16 07/09/16   \n",
       "13              INF-992 MONO         NaN        MARINA  06/24/16 07/04/16   \n",
       "14           YNP-41 MARIPOSA         NaN         LAKES  06/26/16 10/31/16   \n",
       "15          LPF-1986 VENTURA         NaN          PINE  06/30/16 07/17/16   \n",
       "16            KRN-24109 KERN         NaN          DEER  07/01/16 07/10/16   \n",
       "17            KRN-25390 KERN         NaN          FORT  07/08/16 07/09/16   \n",
       "18    LAC-198015 LOS ANGELES         NaN          SAGE  07/09/16 07/16/16   \n",
       "19        MCP-2125 SAN DIEGO         NaN        ROBLAR  07/21/16 07/27/16   \n",
       "20      ANF-3008 LOS ANGELES         NaN          SAND  07/22/16 08/06/16   \n",
       "21             INF-1415 MONO         NaN         CLARK  08/04/16 08/12/16   \n",
       "22  BDF-10205 SAN BERNARDINO         NaN         PILOT  08/07/16 08/16/16   \n",
       "23             INF-1457 INYO         NaN     HORSESHOE  08/09/16 08/17/16   \n",
       "24             SQF-2595 KERN         NaN         CEDAR  08/16/16 09/24/16   \n",
       "25  BDF-10468 SAN BERNARDINO         NaN       BLUECUT  08/16/16 09/08/16   \n",
       "26    LPF-2809 SANTA BARBARA         NaN           REY  08/18/16 09/15/16   \n",
       "27           STF-2257 ALPINE         NaN     MOKELUMNE  08/19/16 09/20/16   \n",
       "28           SQF-2683 TULARE         NaN          TULE  08/22/16 08/22/16   \n",
       "29            KRN-32226 KERN         NaN         RANGE  08/26/16 08/31/16   \n",
       "30         KNF-7501 SISKIYOU         NaN           GAP  08/27/16 09/16/16   \n",
       "\n",
       "   ORIGIN    ACRES BURNED Unnamed: 6 Unnamed: 7    Unnamed: 8  STRUCTURES  \\\n",
       "0     DPA  CAL FIRE OTHER      TOTAL  VEG. TYPE         CAUSE  DEST. DAM.   \n",
       "1     NaN             NaN        NaN        NaN           NaN         NaN   \n",
       "2     MIL           3,712      3,712          G  UNDETERMINED         NaN   \n",
       "3     BLM           1,324      1,324         BG         HUMAN         NaN   \n",
       "4    USFS           3,000      3,000         BG  UNDETERMINED         NaN   \n",
       "5     BLM           2,003      2,003         BG  UNDETERMINED         NaN   \n",
       "6   LOCAL             465        465         BG  UNDETERMINED         9 1   \n",
       "7    USFS           2,520      2,520         BG  UNDETERMINED           1   \n",
       "8    USFS           2,860      2,860         BT  UNDETERMINED         NaN   \n",
       "9    USFS     2,990 4,484      7,474         BG  UNDETERMINED           5   \n",
       "10   USFS           1,146      1,146         BG         HUMAN         NaN   \n",
       "11   USFS           4,253      4,253         BG         HUMAN         NaN   \n",
       "12   USFS          48,019     48,019        BGT  UNDETERMINED      286 12   \n",
       "13   USFS             654        654         BT  UNDETERMINED           1   \n",
       "14    NPS           1,001      1,001         BT     LIGHTNING         NaN   \n",
       "15   USFS           2,304      2,304         BT  UNDETERMINED         NaN   \n",
       "16     CC           1,785      1,785        BGT  UNDETERMINED         NaN   \n",
       "17     CC             554        554         BG  UNDETERMINED         NaN   \n",
       "18     CC         990 110      1,100        BGT      CAMPFIRE           2   \n",
       "19    MIL           1,245      1,245         BG  UNDETERMINED         NaN   \n",
       "20   USFS    2,188 39,195     41,383        BGT  UNDETERMINED      116 20   \n",
       "21   USFS           2,819      2,819        BGT     LIGHTNING         NaN   \n",
       "22   USFS       472 7,638      8,110        BGT  UNDETERMINED         NaN   \n",
       "23   USFS             369        369         BT  UNDETERMINED         NaN   \n",
       "24   USFS          29,322     29,322         BT  UNDETERMINED           6   \n",
       "25   USFS          36,274     36,274        BGT  UNDETERMINED       321 8   \n",
       "26   USFS          32,606     32,606         BG  UNDETERMINED           5   \n",
       "27   USFS             655        655        BGT     LIGHTNING         NaN   \n",
       "28   USFS             395        395         BG  UNDETERMINED         NaN   \n",
       "29     CC             600        600        BGT         HUMAN         NaN   \n",
       "30   USFS          33,867     33,867         BT  UNDETERMINED          14   \n",
       "\n",
       "    FATALITIES  Unnamed: 11  \n",
       "0   FIRE CIVIL          NaN  \n",
       "1          NaN          NaN  \n",
       "2          NaN          NaN  \n",
       "3          NaN          NaN  \n",
       "4          NaN          NaN  \n",
       "5          NaN          NaN  \n",
       "6          NaN          NaN  \n",
       "7          NaN          NaN  \n",
       "8          NaN          NaN  \n",
       "9          NaN          NaN  \n",
       "10         NaN          1.0  \n",
       "11         NaN          NaN  \n",
       "12         NaN          2.0  \n",
       "13         NaN          NaN  \n",
       "14         NaN          NaN  \n",
       "15         NaN          NaN  \n",
       "16         NaN          NaN  \n",
       "17         NaN          NaN  \n",
       "18         NaN          NaN  \n",
       "19         NaN          NaN  \n",
       "20         NaN          1.0  \n",
       "21         NaN          NaN  \n",
       "22         NaN          NaN  \n",
       "23         NaN          NaN  \n",
       "24         NaN          NaN  \n",
       "25         NaN          NaN  \n",
       "26         NaN          NaN  \n",
       "27         NaN          NaN  \n",
       "28         NaN          NaN  \n",
       "29         NaN          NaN  \n",
       "30         NaN          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dirty 2016 page 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ACRES BURNED</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>STRUCTURES</th>\n",
       "      <th>FATALITIES</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INCIDENT #</td>\n",
       "      <td>COUNTY / UNIT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FIRE NAME</td>\n",
       "      <td>START CONT.</td>\n",
       "      <td>DPA</td>\n",
       "      <td>CAL FIRE OTHER</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>VEG. TYPE</td>\n",
       "      <td>CAUSE</td>\n",
       "      <td>DEST. DAM.</td>\n",
       "      <td>FIRE CIVIL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KRN-32375</td>\n",
       "      <td>KERN</td>\n",
       "      <td>HAVILA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08/27/16 08/31/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>304</td>\n",
       "      <td>304</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNF-2154</td>\n",
       "      <td>FRESNO</td>\n",
       "      <td>CROWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/15/16 10/31/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "      <td>T</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFV-3151</td>\n",
       "      <td>SANTA BARBARA</td>\n",
       "      <td>CANYON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/17/16 09/24/16</td>\n",
       "      <td>MIL</td>\n",
       "      <td>12,742</td>\n",
       "      <td>12,742</td>\n",
       "      <td>B</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDF-910</td>\n",
       "      <td>MODOC</td>\n",
       "      <td>SOUP COMPLEX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/17/16 10/31/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,722</td>\n",
       "      <td>2,722</td>\n",
       "      <td>GT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INF-1763</td>\n",
       "      <td>MONO</td>\n",
       "      <td>OWENS RIVER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/17/16 10/15/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>5,443</td>\n",
       "      <td>5,443</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KRN-35570</td>\n",
       "      <td>KERN</td>\n",
       "      <td>FLAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/19/16 09/22/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>G</td>\n",
       "      <td>EQUIPMENT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SQF-3261</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>SLATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/04/16 11/05/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,160</td>\n",
       "      <td>2,160</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNF-2370</td>\n",
       "      <td>FRESNO</td>\n",
       "      <td>SACATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/11/16 10/19/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,100</td>\n",
       "      <td>2,100</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SQF-3384</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>JACOBSON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/20/16 11/01/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>1,702</td>\n",
       "      <td>1,702</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SQF-3456</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>MEADOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/29/16 11/22/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>4,347</td>\n",
       "      <td>4,347</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SQF-3464</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>HIDDEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/02/16 11/23/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,768</td>\n",
       "      <td>2,768</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SUBTOTAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10,190 297,024</td>\n",
       "      <td>307,214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>768 43</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141,718 372,349</td>\n",
       "      <td>514,067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1,291 92</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     Unnamed: 1    Unnamed: 2 Unnamed: 3               DATE  \\\n",
       "0   INCIDENT #  COUNTY / UNIT           NaN  FIRE NAME        START CONT.   \n",
       "1    KRN-32375           KERN        HAVILA        NaN  08/27/16 08/31/16   \n",
       "2     SNF-2154         FRESNO         CROWN        NaN  09/15/16 10/31/16   \n",
       "3     AFV-3151  SANTA BARBARA        CANYON        NaN  09/17/16 09/24/16   \n",
       "4      MDF-910          MODOC  SOUP COMPLEX        NaN  09/17/16 10/31/16   \n",
       "5     INF-1763           MONO   OWENS RIVER        NaN  09/17/16 10/15/16   \n",
       "6    KRN-35570           KERN          FLAT        NaN  09/19/16 09/22/16   \n",
       "7     SQF-3261         TULARE         SLATE        NaN  10/04/16 11/05/16   \n",
       "8     SNF-2370         FRESNO        SACATA        NaN  10/11/16 10/19/16   \n",
       "9     SQF-3384         TULARE      JACOBSON        NaN  10/20/16 11/01/16   \n",
       "10    SQF-3456         TULARE        MEADOW        NaN  10/29/16 11/22/16   \n",
       "11    SQF-3464         TULARE        HIDDEN        NaN  11/02/16 11/23/16   \n",
       "12    SUBTOTAL            NaN           NaN        NaN                NaN   \n",
       "13       TOTAL            NaN           NaN        NaN                NaN   \n",
       "\n",
       "   ORIGIN     ACRES BURNED Unnamed: 7 Unnamed: 8    Unnamed: 9  STRUCTURES  \\\n",
       "0     DPA   CAL FIRE OTHER      TOTAL  VEG. TYPE         CAUSE  DEST. DAM.   \n",
       "1      CC              304        304         BG  UNDETERMINED         NaN   \n",
       "2    USFS              800        800          T     LIGHTNING         NaN   \n",
       "3     MIL           12,742     12,742          B  UNDETERMINED         NaN   \n",
       "4    USFS            2,722      2,722         GT  UNDETERMINED         NaN   \n",
       "5    USFS            5,443      5,443        BGT  UNDETERMINED           4   \n",
       "6      CC              307        307          G     EQUIPMENT         NaN   \n",
       "7    USFS            2,160      2,160         BT     LIGHTNING         NaN   \n",
       "8    USFS            2,100      2,100        BGT  UNDETERMINED         NaN   \n",
       "9    USFS            1,702      1,702         BT  UNDETERMINED         NaN   \n",
       "10   USFS            4,347      4,347         BT     LIGHTNING         NaN   \n",
       "11   USFS            2,768      2,768         BT     LIGHTNING         NaN   \n",
       "12    NaN   10,190 297,024    307,214        NaN           NaN      768 43   \n",
       "13    NaN  141,718 372,349    514,067        NaN           NaN    1,291 92   \n",
       "\n",
       "    FATALITIES  Unnamed: 12  \n",
       "0   FIRE CIVIL          NaN  \n",
       "1          NaN          NaN  \n",
       "2          NaN          NaN  \n",
       "3            1          NaN  \n",
       "4          NaN          NaN  \n",
       "5          NaN          NaN  \n",
       "6          NaN          NaN  \n",
       "7          NaN          NaN  \n",
       "8          NaN          NaN  \n",
       "9          NaN          NaN  \n",
       "10         NaN          NaN  \n",
       "11         NaN          NaN  \n",
       "12           1          4.0  \n",
       "13           2          6.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'clean 2016 page 3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INCIDENT #</th>\n",
       "      <th>COUNTY/UNIT</th>\n",
       "      <th>FIRE_NAME</th>\n",
       "      <th>START_DATE</th>\n",
       "      <th>CONT_DATE</th>\n",
       "      <th>ORIGIN_DPT</th>\n",
       "      <th>BURNED_TOTAL</th>\n",
       "      <th>VEG_TYPE</th>\n",
       "      <th>CAUSE</th>\n",
       "      <th>STRUCT_DEST</th>\n",
       "      <th>STRUCT_DAM</th>\n",
       "      <th>FATALITY_FIRE</th>\n",
       "      <th>FATALITY_CIVIL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KRN-32375</td>\n",
       "      <td>KERN</td>\n",
       "      <td>HAVILA</td>\n",
       "      <td>08/27/16</td>\n",
       "      <td>08/31/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>304</td>\n",
       "      <td>BG</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNF-2154</td>\n",
       "      <td>FRESNO</td>\n",
       "      <td>CROWN</td>\n",
       "      <td>09/15/16</td>\n",
       "      <td>10/31/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>800</td>\n",
       "      <td>T</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFV-3151</td>\n",
       "      <td>SANTA BARBARA</td>\n",
       "      <td>CANYON</td>\n",
       "      <td>09/17/16</td>\n",
       "      <td>09/24/16</td>\n",
       "      <td>MIL</td>\n",
       "      <td>12,742</td>\n",
       "      <td>B</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDF-910</td>\n",
       "      <td>MODOC</td>\n",
       "      <td>SOUP COMPLEX</td>\n",
       "      <td>09/17/16</td>\n",
       "      <td>10/31/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,722</td>\n",
       "      <td>GT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INF-1763</td>\n",
       "      <td>MONO</td>\n",
       "      <td>OWENS RIVER</td>\n",
       "      <td>09/17/16</td>\n",
       "      <td>10/15/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>5,443</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KRN-35570</td>\n",
       "      <td>KERN</td>\n",
       "      <td>FLAT</td>\n",
       "      <td>09/19/16</td>\n",
       "      <td>09/22/16</td>\n",
       "      <td>CC</td>\n",
       "      <td>307</td>\n",
       "      <td>G</td>\n",
       "      <td>EQUIPMENT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SQF-3261</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>SLATE</td>\n",
       "      <td>10/04/16</td>\n",
       "      <td>11/05/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,160</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNF-2370</td>\n",
       "      <td>FRESNO</td>\n",
       "      <td>SACATA</td>\n",
       "      <td>10/11/16</td>\n",
       "      <td>10/19/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,100</td>\n",
       "      <td>BGT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SQF-3384</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>JACOBSON</td>\n",
       "      <td>10/20/16</td>\n",
       "      <td>11/01/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>1,702</td>\n",
       "      <td>BT</td>\n",
       "      <td>UNDETERMINED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SQF-3456</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>MEADOW</td>\n",
       "      <td>10/29/16</td>\n",
       "      <td>11/22/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>4,347</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SQF-3464</td>\n",
       "      <td>TULARE</td>\n",
       "      <td>HIDDEN</td>\n",
       "      <td>11/02/16</td>\n",
       "      <td>11/23/16</td>\n",
       "      <td>USFS</td>\n",
       "      <td>2,768</td>\n",
       "      <td>BT</td>\n",
       "      <td>LIGHTNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INCIDENT #    COUNTY/UNIT     FIRE_NAME START_DATE CONT_DATE ORIGIN_DPT  \\\n",
       "1   KRN-32375           KERN        HAVILA   08/27/16  08/31/16         CC   \n",
       "2    SNF-2154         FRESNO         CROWN   09/15/16  10/31/16       USFS   \n",
       "3    AFV-3151  SANTA BARBARA        CANYON   09/17/16  09/24/16        MIL   \n",
       "4     MDF-910          MODOC  SOUP COMPLEX   09/17/16  10/31/16       USFS   \n",
       "5    INF-1763           MONO   OWENS RIVER   09/17/16  10/15/16       USFS   \n",
       "6   KRN-35570           KERN          FLAT   09/19/16  09/22/16         CC   \n",
       "7    SQF-3261         TULARE         SLATE   10/04/16  11/05/16       USFS   \n",
       "8    SNF-2370         FRESNO        SACATA   10/11/16  10/19/16       USFS   \n",
       "9    SQF-3384         TULARE      JACOBSON   10/20/16  11/01/16       USFS   \n",
       "10   SQF-3456         TULARE        MEADOW   10/29/16  11/22/16       USFS   \n",
       "11   SQF-3464         TULARE        HIDDEN   11/02/16  11/23/16       USFS   \n",
       "\n",
       "   BURNED_TOTAL VEG_TYPE         CAUSE STRUCT_DEST  STRUCT_DAM FATALITY_FIRE  \\\n",
       "1           304       BG  UNDETERMINED         NaN         NaN           NaN   \n",
       "2           800        T     LIGHTNING         NaN         NaN           NaN   \n",
       "3        12,742        B  UNDETERMINED         NaN         NaN             1   \n",
       "4         2,722       GT  UNDETERMINED         NaN         NaN           NaN   \n",
       "5         5,443      BGT  UNDETERMINED           4         NaN           NaN   \n",
       "6           307        G     EQUIPMENT         NaN         NaN           NaN   \n",
       "7         2,160       BT     LIGHTNING         NaN         NaN           NaN   \n",
       "8         2,100      BGT  UNDETERMINED         NaN         NaN           NaN   \n",
       "9         1,702       BT  UNDETERMINED         NaN         NaN           NaN   \n",
       "10        4,347       BT     LIGHTNING         NaN         NaN           NaN   \n",
       "11        2,768       BT     LIGHTNING         NaN         NaN           NaN   \n",
       "\n",
       "    FATALITY_CIVIL  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "5              NaN  \n",
       "6              NaN  \n",
       "7              NaN  \n",
       "8              NaN  \n",
       "9              NaN  \n",
       "10             NaN  \n",
       "11             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO: Update 2000 - 2002 years to new cleaning procedures\n",
    "\n",
    "#2000\n",
    "#page 1\n",
    "#columns are rows, add correct columns, superfluous col\n",
    "clean_2000_1 = largest_2000_1.append(largest_2000_1.columns.to_series(), ignore_index=True)\n",
    "clean_2000_1 = clean_2000_1.drop(['100%', 'Unnamed: 7', '400'], axis=1)\n",
    "clean_2000_1.columns = ROWS_N\n",
    "#page 2\n",
    "#columns are a row, add correct cols, structural cols connected\n",
    "clean_2000_2 = largest_2000_2.dropna(subset=['08/27/00'])\n",
    "clean_2000_2 = clean_2000_2.drop(['Unnamed: 13', 'Unnamed: 7', '8,084'], axis=1)\n",
    "clean_2000_2 = clean_2000_2.append(clean_2000_2.columns.to_series(), ignore_index=True)\n",
    "clean_2000_2['x'] = np.nan\n",
    "clean_2000_2['y'] = np.nan\n",
    "clean_2000_2.columns = ROWS_N\n",
    "clean_2000_2['INCIDENT #'] = clean_2000_2['INCIDENT #'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "#2001\n",
    "#page 1\n",
    "# columns and row 0 useless, col 11 useless\n",
    "# structure and fatality rows are stuck\n",
    "# drop acres burned broken (except total)\n",
    "clean_2001_1 = largest_2001_1.drop(0).drop(['Unnamed: 11', '%', 'ACRES BURNED'], axis=1)\n",
    "clean_2001_1 = column_separator(clean_2001_1, 'STRUCTURES', STRUCT_ROW)\n",
    "clean_2001_1 = column_separator(clean_2001_1, 'FATALITIES', FATAL_ROW)\n",
    "clean_2001_1.columns = ROWS_N\n",
    "clean_2001_1\n",
    "# page 2\n",
    "#columns and row 0 useless, % col 3, 13 useless\n",
    "#drop nan start date\n",
    "#connected columns : unit/name structures fatalities\n",
    "# TODO: UNIT AND FIRE NAME\n",
    "clean_2001_2 = largest_2001_2.drop(0).drop(['%', 'Unnamed: 3', 'ACRES BURNED', 'Unnamed: 13'], axis = 1)\n",
    "clean_2001_2 = clean_2001_2.dropna(subset=['DATE'])\n",
    "clean_2001_2.insert(2, 'placeholder', -1)\n",
    "clean_2001_2 = column_separator(clean_2001_2, 'STRUCTURES', STRUCT_ROW)\n",
    "clean_2001_2 = column_separator(clean_2001_2, 'FATALITIES', FATAL_ROW)\n",
    "clean_2001_2.columns = ROWS_N\n",
    "clean_2001_2\n",
    "\n",
    "#2002\n",
    "#page 1\n",
    "#column is row\n",
    "#connected columns: unit/name\n",
    "# TODO: UNIT AND FIRE NAME\n",
    "clean_2002_1 = largest_2002_1.drop(['100%', '320', 'Unnamed: 8'], axis=1)\n",
    "clean_2002_1 = clean_2002_1.append(clean_2002_1.columns.to_series(), ignore_index=True)\n",
    "clean_2002_1.columns = ROWS_N\n",
    "clean_2002_1\n",
    "#page 2\n",
    "#column is row\n",
    "clean_2002_2 = largest_2002_2.drop(['100%', 'Unnamed: 7', '670'], axis=1)\n",
    "clean_2002_2 = clean_2002_2.append(clean_2002_2.columns.to_series(), ignore_index=True)\n",
    "clean_2002_2.columns = ROWS_N\n",
    "clean_2002_2\n",
    "#page 3\n",
    "#column is row\n",
    "#drop na dates\n",
    "#combined columns: name/date structure fatality\n",
    "clean_2002_3 = largest_2002_3.drop(['Unnamed: 6', '1,200'], axis=1)\n",
    "clean_2002_3 = column_separator(clean_2002_3, 'Unnamed: 11', STRUCT_ROW)\n",
    "clean_2002_3 = column_separator(clean_2002_3, 'Unnamed: 12', FATAL_ROW)\n",
    "clean_2002_3 = clean_2002_3.dropna(subset=['Mountain 09/09/02'])\n",
    "clean_2002_3.columns = ROWS_N\n",
    "#procedure for separating firename and date\n",
    "clean_2002_3.START_DATE = clean_2002_3.FIRE_NAME.apply(lambda x: x.split(' ')[-1])\n",
    "clean_2002_3.FIRE_NAME = clean_2002_3.FIRE_NAME.apply(lambda x: ' '.join(x.split(' ')[:-1]))\n",
    "\n",
    "#END TODO\n",
    "\n",
    "#2003\n",
    "#page 1\n",
    "largest_2003_1\n",
    "clean_2003_1 = robust_col_sep(largest_2003_1, 0, ['apples', 'bannanas'])\n",
    "clean_2003_1 = robust_col_sep(clean_2003_1, 4, ['oranges', 'strawberries'])\n",
    "clean_2003_1 = clean_2003_1.drop(clean_2003_1.columns[[2, 6, 8, 9, 13]], axis=1)\n",
    "clean_2003_1 = clean_2003_1.drop(0)\n",
    "clean_2003_1 = robust_col_sep(clean_2003_1, 9, ['mangos', 'peaches'])\n",
    "clean_2003_1 = robust_col_sep(clean_2003_1, 11, ['grapefruit', 'lemons'])\n",
    "#clean_2003_1 = clean_2003_1.drop(clean_2003_1.columns[[1]], axis=1)\n",
    "clean_2003_1.columns = ROWS_N\n",
    "#page 2\n",
    "clean_2003_2 = robust_col_sep(largest_2003_2, 0, ['1', '2']) \n",
    "clean_2003_2 = robust_col_sep(clean_2003_2, 4, ['3', '4'])\n",
    "clean_2003_2 = robust_col_sep(clean_2003_2, 13, ['13', '14'])\n",
    "clean_2003_2 = robust_col_sep(clean_2003_2, 15, ['15', '16'])\n",
    "clean_2003_2 = robust_col_sep(clean_2003_2, 11, ['11', '12'])\n",
    "clean_2003_2 = clean_2003_2.drop(0)\n",
    "clean_2003_2 = clean_2003_2.drop(columns=clean_2003_2.columns[[3, 6, 8, 9, 13]])\n",
    "clean_2003_2.columns = ROWS_N\n",
    "#page 3\n",
    "clean_2003_3 = largest_2003_3.drop(range(7,12))\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 0, [0, 1])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 5, [5, 6])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 9, [9, 10])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 14, [14, 15])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 16, [16, 17])\n",
    "clean_2003_3 = clean_2003_3.drop(columns=clean_2003_3.columns[[2, 4, 8, 10, 11]])\n",
    "clean_2003_3 = clean_2003_3.drop(0)\n",
    "clean_2003_3.columns = ROWS_N\n",
    "\n",
    "#2004\n",
    "#page1\n",
    "clean_2004_1 = robust_col_sep(largest_2004_1, 0, ['0','1'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 4, ['4','5'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 5, ['5','6'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 8, ['8','9'], front=False)\n",
    "#special case\n",
    "abomination = clean_2004_1.STRUCTURESFATALITIES\n",
    "clean_2004_1['12'] = abomination.apply(lambda x: x.split(' ')[0])\n",
    "clean_2004_1['13'] = abomination.apply(lambda x: x.split(' ')[1])\n",
    "clean_2004_1['14'] = abomination.apply(lambda x: x.split(' ')[2])\n",
    "clean_2004_1['15'] = abomination.apply(lambda x: x.split(' ')[3])\n",
    "clean_2004_1 = clean_2004_1.drop(['STRUCTURESFATALITIES'], axis=1)\n",
    "# back to regular programing\n",
    "clean_2004_1 = clean_2004_1.drop(0)\n",
    "clean_2004_1 = clean_2004_1.drop(columns=clean_2004_1.columns[[2, 7, 8]])\n",
    "clean_2004_1.columns = ROWS_N\n",
    "#page2\n",
    "clean_2004_2 = robust_col_sep(largest_2004_2, 0, ['0', '1'])\n",
    "clean_2004_2 = robust_col_sep(clean_2004_2, 4, ['4', '5'])\n",
    "clean_2004_2 = robust_col_sep(clean_2004_2, 5, ['5', '6'])\n",
    "clean_2004_2 = robust_col_sep(clean_2004_2, 8, ['8', '9'])\n",
    "#another abomination\n",
    "abomination = clean_2004_2.STRUCTURESFATALITIE\n",
    "clean_2004_2['12'] = abomination.apply(lambda x: x.split(' ')[0])\n",
    "clean_2004_2['13'] = abomination.apply(lambda x: x.split(' ')[1])\n",
    "clean_2004_2['14'] = abomination.apply(lambda x: x.split(' ')[2])\n",
    "clean_2004_2['15'] = 0\n",
    "clean_2004_2 = clean_2004_2.drop(['STRUCTURESFATALITIE'], axis=1)\n",
    "#end special case\n",
    "clean_2004_2 = clean_2004_2.drop([0, 38])\n",
    "clean_2004_2 = clean_2004_2.drop(columns=clean_2004_2.columns[[3, 7, 8]])\n",
    "clean_2004_2.columns = ROWS_N\n",
    "#37 pages left\n",
    "\n",
    "#2005\n",
    "\n",
    "# 2015\n",
    "# Clean the 2015_page_1\n",
    "clean_2015_1 = largest_2015_1\n",
    "clean_2015_1 = clean_2015_1.drop(0)\n",
    "clean_2015_1.columns = ROWS_N\n",
    "clean_2015_1['ORIGIN_DPT'] = clean_2015_1['BURNED_TOTAL']\n",
    "clean_2015_1['BURNED_TOTAL'] = clean_2015_1['VEG_TYPE']\n",
    "clean_2015_1['VEG_TYPE'] = clean_2015_1['STRUCT_DEST']\n",
    "clean_2015_1['CAUSE'] = clean_2015_1['STRUCT_DAM']\n",
    "clean_2015_1 = column_separator(clean_2015_1, \"FATALITY_FIRE\", STRUCT_ROW)\n",
    "clean_2015_1['FATALITY_FIRE'] = np.nan\n",
    "# Reorder the columns\n",
    "clean_2015_1 = clean_2015_1.reindex(columns=ROWS_N)\n",
    "# Refill missing data (Some values lost during column separation)\n",
    "clean_2015_1.iloc[3, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 2\n",
    "clean_2015_1.iloc[11, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 16\n",
    "clean_2015_1.iloc[13, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 7\n",
    "clean_2015_1.iloc[15, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 27\n",
    "clean_2015_1.iloc[17, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 1\n",
    "clean_2015_1.iloc[18, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 1\n",
    "clean_2015_1.iloc[20, clean_2015_1.columns.get_loc('STRUCT_DEST')] = 965\n",
    "\n",
    "# Clean the 2015_page_2 \n",
    "clean_2015_2 = largest_2015_2.drop(0)\n",
    "clean_2015_2.columns = ROWS_N\n",
    "clean_2015_2['ORIGIN_DPT'] = clean_2015_2['BURNED_TOTAL']\n",
    "clean_2015_2['BURNED_TOTAL'] = clean_2015_2['VEG_TYPE']\n",
    "clean_2015_2['VEG_TYPE'] = clean_2015_2['STRUCT_DEST']\n",
    "clean_2015_2['CAUSE'] = clean_2015_2['STRUCT_DAM']\n",
    "clean_2015_2 = column_separator(clean_2015_2, \"FATALITY_FIRE\", STRUCT_ROW)\n",
    "clean_2015_2['FATALITY_FIRE'] = clean_2015_2['FATALITY_CIVIL']\n",
    "# Reorder the columns\n",
    "clean_2015_2 = clean_2015_2.reindex(columns=ROWS_N)\n",
    "# Crosschecking dataframe data manually\n",
    "clean_2015_2['FATALITY_CIVIL'] = np.nan\n",
    "clean_2015_2.iloc[6, clean_2015_2.columns.get_loc('STRUCT_DEST')] = 4\n",
    "clean_2015_2.iloc[8, clean_2015_2.columns.get_loc('STRUCT_DAM')] = 1\n",
    "# Fill columns 14-18 with right values for STRUCT_DEST column\n",
    "clean_2015_2.iloc[14:19, clean_2015_2.columns.get_loc('STRUCT_DEST')] = largest_2015_2['Structures'][15:20]\n",
    "\n",
    "# Clean the 2015_page_3\n",
    "# Note do we keep TOTAL?\n",
    "clean_2015_3 = largest_2015_3.drop(0)\n",
    "# Drop the total row\n",
    "clean_2015_3 = clean_2015_3.drop(8)\n",
    "clean_2015_3.columns = ROWS_N\n",
    "clean_2015_3['ORIGIN_DPT'] = clean_2015_3['BURNED_TOTAL']\n",
    "clean_2015_3['BURNED_TOTAL'] = clean_2015_3['VEG_TYPE']\n",
    "clean_2015_3['VEG_TYPE'] = clean_2015_3['STRUCT_DEST']\n",
    "clean_2015_3['CAUSE'] = clean_2015_3['STRUCT_DAM']\n",
    "clean_2015_3 = column_separator(clean_2015_3, \"FATALITY_FIRE\", STRUCT_ROW)\n",
    "clean_2015_3['FATALITY_FIRE'] = np.nan\n",
    "# Reorder the columns\n",
    "clean_2015_3 = clean_2015_3.reindex(columns=ROWS_N)\n",
    "clean_2015_3.iloc[1, clean_2015_3.columns.get_loc('STRUCT_DEST')] = 3\n",
    "\n",
    "# 2016\n",
    "clean_2016_1 = largest_2016_1.drop(0)\n",
    "clean_2016_1 = clean_2016_1.drop(1)\n",
    "clean_2016_1.columns = ROWS_N\n",
    "# Split the incident number and COUNTY/UNIT\n",
    "clean_2016_1['COUNTY/UNIT'] = clean_2016_1['INCIDENT #'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
    "clean_2016_1['INCIDENT #'] = clean_2016_1['INCIDENT #'].apply(lambda x: x.split(' ')[0])\n",
    "# Drop NAN row\n",
    "clean_2016_1 = clean_2016_1.drop(31)\n",
    "# Separate the date\n",
    "clean_2016_1['START_DATE'] = clean_2016_1['CONT_DATE'].apply(lambda x: x.split(' ')[0])\n",
    "clean_2016_1['CONT_DATE'] = clean_2016_1['CONT_DATE'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
    "# Reassign columns\n",
    "clean_2016_1['BURNED_TOTAL'] = clean_2016_1['VEG_TYPE']\n",
    "clean_2016_1['VEG_TYPE'] = clean_2016_1['CAUSE']\n",
    "clean_2016_1['CAUSE'] = clean_2016_1['STRUCT_DEST']\n",
    "# Split structure data\n",
    "clean_2016_1 = column_separator(clean_2016_1, \"STRUCT_DAM\", [\"STRUCT_DEST\",\"TEMP_DAM\"])\n",
    "clean_2016_1['STRUCT_DAM'] = clean_2016_1['TEMP_DAM']\n",
    "clean_2016_1 = clean_2016_1.drop(columns=[\"TEMP_DAM\"], axis=1)\n",
    "# Restructure data\n",
    "clean_2016_1 = clean_2016_1.reindex(columns=ROWS_N)\n",
    "\n",
    "# Fill data after cross-checking\n",
    "# -2 since 2 columnds dropped\n",
    "clean_2016_1.iloc[24, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 7\n",
    "clean_2016_1.iloc[22, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 3\n",
    "clean_2016_1.iloc[21, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 1\n",
    "clean_2016_1.iloc[20, clean_2016_1.columns.get_loc('STRUCT_DAM')] = 1\n",
    "clean_2016_1.iloc[18, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 3\n",
    "clean_2016_1.iloc[14, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 2\n",
    "clean_2016_1.iloc[13, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 2\n",
    "clean_2016_1.iloc[9, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 2\n",
    "clean_2016_1.iloc[7, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 1\n",
    "\n",
    "# Page 2 for 2016 dataset\n",
    "# Drop first two columns\n",
    "clean_2016_2 = largest_2016_2.drop(0)\n",
    "clean_2016_2 = clean_2016_2.drop(1)\n",
    "clean_2016_2['TEMP'] = np.nan\n",
    "# Drop last column: SUBTOTAL COLUMN (3 columns dropped)\n",
    "clean_2016_2.columns = ROWS_N\n",
    "\n",
    "# Split the INCIDENT NUM to extract COUNTY/UNIT\n",
    "clean_2016_2['COUNTY/UNIT'] = clean_2016_2['INCIDENT #'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
    "clean_2016_2['INCIDENT #'] = clean_2016_2['INCIDENT #'].apply(lambda x: x.split(' ')[0])\n",
    "# MOVE CONT_DATE TO TEMP\n",
    "clean_2016_2['TEMP'] = clean_2016_2['CONT_DATE']\n",
    "# Split the DATES (stuck at START_DATE COLUMNN)\n",
    "clean_2016_2['CONT_DATE'] = clean_2016_2['START_DATE'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
    "clean_2016_2['START_DATE'] = clean_2016_2['START_DATE'].apply(lambda x: x.split(' ')[0])\n",
    "# Set ORIGIN_DPT with its values held at TEMP \n",
    "clean_2016_2['ORIGIN_DPT'] = clean_2016_2['TEMP']\n",
    "# Split the STRUCT_DEST\n",
    "clean_2016_2 = column_separator(clean_2016_2, \"STRUCT_DEST\", [\"TEMP\", \"STRUCT_DAM\"])\n",
    "# Swap FATAL_FIRE and FATAL_CIVIL\n",
    "clean_2016_2[\"FATALITY_CIVIL\"] = clean_2016_2[\"FATALITY_FIRE\"]\n",
    "clean_2016_2[\"FATALITY_FIRE\"] = np.nan\n",
    "clean_2016_2 = clean_2016_2.rename(index=str, columns={\"TEMP\": \"STRUCT_DEST\"})\n",
    "# Reorder the columns\n",
    "clean_2016_2 = clean_2016_2.reindex(columns=ROWS_N)\n",
    "# Crosscheck data and fill in (-2 cols)\n",
    "clean_2016_2.iloc[28, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 14\n",
    "clean_2016_2.iloc[24, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 5\n",
    "clean_2016_2.iloc[22, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 6\n",
    "clean_2016_2.iloc[16, clean_2016_1.columns.get_loc('STRUCT_DAM')] = 2\n",
    "clean_2016_2.iloc[11, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 1\n",
    "clean_2016_2.iloc[7, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 5\n",
    "clean_2016_2.iloc[5, clean_2016_1.columns.get_loc('STRUCT_DEST')] = 1\n",
    "\n",
    "# Clean 2016_page_3\n",
    "# Drop unwanted columns\n",
    "clean_2016_3 = largest_2016_3.drop(0)\n",
    "clean_2016_3 = clean_2016_3.drop(12)\n",
    "clean_2016_3 = clean_2016_3.drop(13)\n",
    "clean_2016_3.columns = ROWS_N\n",
    "\n",
    "# Split the dates (COMBINED ON CONT_DATE)\n",
    "clean_2016_3['START_DATE'] = clean_2016_3['CONT_DATE'].apply(lambda x: x.split(' ')[0])\n",
    "clean_2016_3['CONT_DATE'] = clean_2016_3['CONT_DATE'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
    "# Reassign column values\n",
    "clean_2016_3['VEG_TYPE'] = clean_2016_3['CAUSE']\n",
    "clean_2016_3['CAUSE'] = clean_2016_3['STRUCT_DEST']\n",
    "clean_2016_3['STRUCT_DEST'] = clean_2016_3['STRUCT_DAM']\n",
    "clean_2016_3['STRUCT_DAM'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATACLEANING TUTORIAL\n",
    "\n",
    "The PDF data cleaning will consist mainly of 4 key procedures:\n",
    "1. Separating columns that are 'stuck' together using `robust_col_sep`\n",
    "    - Note that `COUNTY/UNIT` and `FIRE_NAME` columns can NOT be separated using `robust_col_sep`\n",
    "\n",
    "\n",
    "2. Dropping any columns with acres burned that is NOT `BURNED_TOTAL`\n",
    "    - The two types of of burned columns to delete are `CDF` and `Other`, these names are usually found in row 0\n",
    "    \n",
    "\n",
    "3. Dropping null columns and useless rows\n",
    "    - For dropping rows, there may be a 'totals' row or some text rows at the end of the DataFrame, remove these too\n",
    "    \n",
    "\n",
    "4. Setting correct column names using `df.columns = ROWS_N`\n",
    "    - If you get a length mismatch error, make sure all columns are separated \n",
    "    - If `COUNTY/UNIT` and `FIRE_NAME` are connected, keep the NaN column next to the combined column in order to prevent a length mismatch error. Make sure the order of the rows makes sense! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLES\n",
    "\n",
    "### EXAMPLE 1\n",
    "For the first example I will use the `largest_2003_3` PDF. We want out final prouct to look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(clean_2003_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the dataframe starts like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(largest_2003_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Separating columns that are stuck together\n",
    "\n",
    "A look at the DataFrame reveals the following columns are stuck together:\n",
    "\n",
    "* `INCIDENT #` & `COUTNY/UNIT`\n",
    "* `START_DATE` & `CONT_DATE`\n",
    "* `Other` & `BURNED_TOTAL`\n",
    "* both `STRUCTURES` columns\n",
    "* both `FATALITIES` columns\n",
    "\n",
    "lets separate them\n",
    "\n",
    "NOTE: Check the DataFrame after every call in order to get the right index for the columns becuase you add a column on each call!\n",
    "\n",
    "NOTE2: I recommend making the new names the index that the columns will be in, this will help later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2003_3 = robust_col_sep(largest_2003_3, 0, [0, 1])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 5, [5, 6])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 9, [9, 10])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 14, [14, 15])\n",
    "clean_2003_3 = robust_col_sep(clean_2003_3, 16, [16, 17])\n",
    "clean_2003_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. & 3. Dropping other burned acres, NaN columns, and useless rows\n",
    "\n",
    "Some notes about the code:\n",
    "* `df.drop` has a `columns` keyword that is the same as doing `axis=1` in order to drop columns\n",
    "* the form `df.columns[[number list]]` drops the columns at that index, this helps so you dont have to write the whole name down \n",
    "* we have some text rows and a total row at the bottom we can delete\n",
    "* the `range(start, end)` code snippet makes an array with the range \\[start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2003_3 = clean_2003_3.drop(columns=clean_2003_3.columns[[2, 4, 8, 10, 11]])\n",
    "clean_2003_3 = clean_2003_3.drop(0)\n",
    "clean_2003_3 = clean_2003_3.drop(range(7,12))\n",
    "clean_2003_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add correct column names\n",
    "\n",
    "This is probably the easiest part if everything else was done right just use `df.columns = ROWS_N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2003_3.columns = ROWS_N\n",
    "clean_2003_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE 2\n",
    "\n",
    "For the second example I will use `largest_2004_1`\n",
    "\n",
    "We want the DataFrame to look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(clean_2004_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the table starts like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(largest_2004_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Separating columns stuck together\n",
    "\n",
    "The following columns are stuck together\n",
    "* `INCIDENT #` & `COUNTY/UNIT`\n",
    "* `START_DATE` & `CONT_DATE` & `ORIGIN_DPT`\n",
    "* `CDF_BURNED` & `OTHER_BURNED` & `BURNED_TOTAL`\n",
    "* both `STRUCTURE` cols & both `FATALITIES` cols\n",
    "\n",
    "Some notes about the code:\n",
    "* You can call `robust_col_sep` on the 'same' column multiple times\n",
    "* Setting new column names is particularly useful here for applying column separator multiple times on the 'same' column\n",
    "* In the last function call, notice the front keyword being used, we know that total will always be the third or last value in the grouped totals, therefore we set front to false in order to isolate the element in the back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2004_1 = robust_col_sep(largest_2004_1, 0, ['0','1'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 4, ['4','5'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 5, ['5','6'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 8, ['8','9'], front=False)\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 12, ['12', '13'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 13, ['13', '14'])\n",
    "clean_2004_1 = robust_col_sep(clean_2004_1, 14, ['14', '15'])\n",
    "clean_2004_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. & 3. Dropping other burned acres, NaN columns, and useless rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2004_1 = clean_2004_1.drop(0)\n",
    "clean_2004_1 = clean_2004_1.drop(columns=clean_2004_1.columns[[2, 7, 8]])\n",
    "clean_2004_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add correct column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2004_1.columns = ROWS_N\n",
    "clean_2004_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifact Separation and COUNTY/UNIT and FIRE_NAME separation\n",
    "\n",
    "#### Artifacts\n",
    "* regex separator? most artifacts happen with numbers and letters\n",
    "\n",
    "#### COUNTY/UNIT and FIRE_NAME separation\n",
    "* make a list of actual COUNTY/UNIT names and see if combinations of the combined columns match any values we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combined dataset, this will grow as more datasets are cleaned\n",
    "clean_df = pd.concat([clean_2000_1, clean_2000_2,\n",
    "                      clean_2001_1, clean_2001_2,\n",
    "                      clean_2002_1, clean_2002_2, clean_2002_3,\n",
    "                      clean_2003_1, clean_2003_2, clean_2003_2,\n",
    "                      clean_2004_1, clean_2004_2], ignore_index=True, sort=False)\n",
    "clean_df['START_DATE'] = clean_df.START_DATE.astype(str).apply(datetime_convert_pdf)\n",
    "clean_df['CONT_DATE'] = clean_df.CONT_DATE.astype(str).apply(datetime_convert_pdf)\n",
    "clean_df['BURNED_TOTAL'] = clean_df.BURNED_TOTAL.apply(int_convert)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = clean_df.merge(fires, left_on=['BURNED_TOTAL', 'START_DATE', 'CONT_DATE'], right_on=['FIRE_SIZE', 'START_DATE', 'END_DATE'])\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Sections\n",
    "\n",
    "While not data sets are added (yet!), the coding for other sections can start since these are the final columns kept, as more data is added, the code should mould to the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folium map, to view open map.html\n",
    "m = folium.Map(location = [36, -119])\n",
    "lats = joined.LATITUDE\n",
    "longs = joined.LONGITUDE\n",
    "for i in np.arange(len(lats)):\n",
    "    folium.Marker(location=[lats[i], longs[i]], icon=folium.Icon(color='red', icon='fire')).add_to(m)\n",
    "m.save('map.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
